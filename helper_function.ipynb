{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "from gensim.models import CoherenceModel, TfidfModel, LdaMulticore, Word2Vec, KeyedVectors\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import random \n",
    "import re\n",
    "from collections import namedtuple\n",
    "\n",
    "# Ignore warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy for lemmatization and stop words\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Add additional stop words to Spacy\n",
    "custom_stop_words = ['good', 'great', 'love', 'eat', 'try', 'amazing', 'come', 'food', 'place', 'order', \n",
    "                     'service', 'time', 'definitely', 'outstanding', 'restaurant', 'like', 'get', 'nice', \n",
    "                     'go', 'excellent', 'serve', 'sauce', 'bad', 'price']\n",
    "\n",
    "for word in custom_stop_words:\n",
    "    stop_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "attributes_df = pd.read_csv('./dataset/categories.csv', index_col=0)\n",
    "\n",
    "# Organize cuisine dataframes by utilizing dictionary.\n",
    "categories_dict = {}\n",
    "cols = attributes_df.columns.tolist()\n",
    "dfs = [] # Create master cuisine list\n",
    "\n",
    "# Gather cuisine and key it by cuisine type onto dictionary\n",
    "for col in cols:\n",
    "    category_df = attributes_df[attributes_df[col] == 1]\n",
    "    categories_dict[col] = category_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTION_DICT = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_matrix_df(df, vocab=None):\n",
    "    '''\n",
    "    Creates tf-idf matrix dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Yelp's review or tip dataframe\n",
    "    vocab (optional): list\n",
    "        list of words used for defining vocabular in CountVectorizer\n",
    "    Returns\n",
    "    -------\n",
    "    dtm_tfidf_df : pandas.DataFrame\n",
    "        tf-idf dataframe with Yelp's business id as row index\n",
    "    '''\n",
    "    \n",
    "    # Instantiation\n",
    "    if vocab != None:\n",
    "        vectorizer = CountVectorizer(min_df=.01, # min_df - ignore terms that appear less than 1% of the documents\n",
    "                                     max_df=0.5, # max_df - ignore terms that appear in more than 65% of documents\n",
    "                                     vocabulary=vocab) # ngram_range - consider unigrams, bigrams, trigrams and so forth\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(min_df=.1, # min_df - ignore terms that appear less than 1% of the documents\n",
    "                                     max_df=0.5, # max_df - ignore terms that appear in more than 65% of documents\n",
    "                                     ngram_range=(1,3)) # ngram_range - consider unigrams, bigrams, trigrams and so forth\n",
    "    \n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    \n",
    "    X = vectorizer.fit_transform(df['item_list'])\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "\n",
    "    # Create document term matrix dataframe\n",
    "    dtm_df = pd.DataFrame(X.toarray(), index = df['cuisine'], columns = vocab)\n",
    "\n",
    "    # Create tfidf matrix\n",
    "    X_tfidf = tfidf_transformer.fit_transform(dtm_df)\n",
    "    dtm_tfidf_df = pd.DataFrame(X_tfidf.toarray(), index = df['cuisine'], columns = vocab)\n",
    "    \n",
    "    return dtm_tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_menu_links(location_list):\n",
    "    '''\n",
    "    Collects menu links from allmenus.com - which is collected using state.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    location_list: list of objects\n",
    "        List of objects containing state and cities per object.\n",
    "    Returns\n",
    "    -------\n",
    "    links: list of str\n",
    "        allmenus' url links\n",
    "    rest_titles: list of str\n",
    "        Restaurant name/title\n",
    "    '''\n",
    "    cuisines = ['greek', 'south-american', 'filipino', 'indian', 'jamaican', 'spanish', 'italian', 'mexican', \n",
    "                'chinese', 'british-traditional', 'thai', 'vietnamese', 'brazilian', 'french', 'japanese', 'irish', \n",
    "                'korean', 'moroccan', 'russian']\n",
    "    \n",
    "    links = [] # Note: list for restaurant links\n",
    "    cuisine_list = []\n",
    "    rest_titles = []\n",
    "    \n",
    "    for location in location_list:\n",
    "        \n",
    "        state = location['state']\n",
    "        cities = location['cities']\n",
    "\n",
    "        for city in cities:\n",
    "\n",
    "            # Get restaurants based on cuisines\n",
    "            for cuisine in cuisines:\n",
    "\n",
    "                page = requests.get(f'https://www.allmenus.com/{state}/{city}/-/{cuisine}/')\n",
    "                soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "                # Get all restaurant titles and cuisine list\n",
    "                h4_elems = soup.find_all('h4', class_='name')\n",
    "                p_elems = soup.find_all('p', class_='cousine-list')\n",
    "\n",
    "                i = 0 # iterator for p tags in getting cuisine list\n",
    "\n",
    "                # Loop through all h4 tags for child a tag's hrefs\n",
    "                for elem in h4_elems: \n",
    "\n",
    "                    a_elem = elem.find_all('a')[0] # Get child a tag\n",
    "                    cuisine_type = p_elems[i].getText() # Get cuisine type\n",
    "\n",
    "                    link = a_elem.get('href') # Get href\n",
    "                    links.append(f'{link}%{cuisine_type}')\n",
    "                    rest_titles.append(a_elem.getText())\n",
    "                    i += 1\n",
    "\n",
    "    # Remove duplicate restaurants\n",
    "    return (links, rest_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_menu_df(links, rest_titles):\n",
    "    '''\n",
    "    Builds menu dataframe\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    links: list of str\n",
    "        allmenus' url links to restaurant menu list\n",
    "    rest_titles: list of str\n",
    "        Restaurant name/title\n",
    "    Returns\n",
    "    -------\n",
    "    menu_df: pandas.DataFrame\n",
    "        DataFrame with restaurant name, menu title, menu description, and categories columns\n",
    "    '''\n",
    "    # Collect menu data per restaurant\n",
    "    ids = []\n",
    "    rest_names = []\n",
    "    categories = []\n",
    "    menu_titles = []\n",
    "    menu_desc = []\n",
    "\n",
    "    j = 0 # Iterator for retrieving cuisine list\n",
    "    \n",
    "    # Looping through each restaurant\n",
    "    for link in links:\n",
    "\n",
    "        # Retrieve city\n",
    "        state = link.split('/')[1]\n",
    "        start = f'/{state}/'\n",
    "        end = '/'\n",
    "        city = link.split(start)[1].split(end)[0]\n",
    "\n",
    "        # Get restauarnt id\n",
    "        start = f'/{state}/{city}/'\n",
    "        end = '-'\n",
    "        rest_id = int(link.split(start)[1].split(end)[0])\n",
    "\n",
    "        # Get food categories - 'asian, american, indian etc.'\n",
    "        idx = link.find('%')\n",
    "        category = link[idx + 1:]\n",
    "\n",
    "        # Remove category at the end of the link\n",
    "        link = link[:idx] \n",
    "\n",
    "        # Get restaurant page\n",
    "        page = requests.get(f'https://www.allmenus.com{link}')\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        # Get restaurant name\n",
    "        rest_name = rest_titles[j]\n",
    "        j += 1\n",
    "\n",
    "        # Get menu items and its descriptions\n",
    "        item_titles = soup.find_all('span', class_='item-title')\n",
    "        item_desc = soup.find_all('p', class_='description')\n",
    "        i = 0 # iterator for restaurant ids\n",
    "\n",
    "        # Loop menu titles\n",
    "        for item_title in item_titles:\n",
    "\n",
    "            # Add restaurant id, name, categories, menu title, menu description\n",
    "            ids.append(rest_id)\n",
    "            rest_names.append(rest_name)\n",
    "            categories.append(category)\n",
    "            menu_titles.append(item_title.getText())\n",
    "            \n",
    "            if len(item_desc) != 0:\n",
    "                menu_desc.append(item_desc[i].getText())\n",
    "            i +=1\n",
    "    \n",
    "    # Create menus dataframe with collected data\n",
    "    d = {'id': ids, 'name': rest_names, 'menu_titles': menu_titles, 'menu_desc': menu_desc, 'categories': categories}\n",
    "    menu_df = pd.DataFrame(data=d)\n",
    "    \n",
    "    return menu_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_dict=CONTRACTION_DICT):\n",
    "    \"\"\"\n",
    "    Expands contractions. For example, \"y'all can't\" => \"you cannot\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Text data\n",
    "    Returns\n",
    "    -------\n",
    "    expanded_text: str\n",
    "        Returns expanded text\n",
    "    \"\"\"\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_dict.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_dict.get(match)\\\n",
    "                                if contraction_dict.get(match)\\\n",
    "                                else contraction_dict.get(match.lower())\n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        \n",
    "        return expanded_contraction.replace('-pron-', '').strip()\n",
    "    \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    \n",
    "    return expanded_text.replace('-pron-', '').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Removes special characters\\whitespaces, lowercases, tokenize, filter based on stop words \n",
    "    and lemmatize.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Yelp review or tips text data\n",
    "    Returns\n",
    "    -------\n",
    "    clean_text: str\n",
    "        Returns tokenized text\n",
    "    \"\"\"\n",
    "    tokens = nlp(text, disable=['parser', 'ner'])\n",
    "    #pos_tags = ['NOUN', 'VERB', 'ADJ', 'ADV'] # allowed parts of speech\n",
    "    i = 0 # iterator to store lemmatized and clean text back to dataframe\n",
    "    clean_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        \n",
    "        if (not token.is_punct # no punctuation\n",
    "            and not token.is_space # no whitespace\n",
    "            and token.is_alpha): # include alphabets\n",
    "            #and token.pos_ in pos_tags): # include noun, verb, adjectives, adverbs\n",
    "            \n",
    "            if str(token) not in stop_words: # no stop words\n",
    "\n",
    "                word = token.lemma_.strip().lower() # lemmatize, whitespace and lowercase\n",
    "                clean_tokens.append(word)\n",
    "                \n",
    "    clean_text = ' '.join(clean_tokens) # re-create text from clean tokens\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrams_trigrams_list(text):\n",
    "    '''\n",
    "    Builds bigram and trigram models and returns list of bigrams and trigrams list\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    text:  pandas.Series\n",
    "        Text values from Consolidated Dataframe (reviews, tips, menu ingredient, etc.)\n",
    "    Returns\n",
    "    -------\n",
    "    bigram and trigram: list (str)\n",
    "        list of bigrams and trigrams\n",
    "    '''\n",
    "    text_data = [] # Storing each tokens\n",
    "    text.apply(lambda text: text_data.append(text.split(' '))); # Split by individual words\n",
    "    \n",
    "    # Build bigram and trigram models\n",
    "    bigram_model, trigram_model = build_bigram_trigram_models(text_data)\n",
    "    \n",
    "    # Build bigrams and trigrams list\n",
    "    bigrams_list, trigrams_list = build_bigram_trigram_lists(text_data, bigram_model, trigram_model)\n",
    "    \n",
    "    return bigrams_list, trigrams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigram_trigram_models(text_data):\n",
    "    '''\n",
    "    Builds bigram and trigram models\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    text_data:  list of tokens\n",
    "        tokens from reviews/tips text column\n",
    "    Returns\n",
    "    -------\n",
    "    bigram_model and trigram_model: gensim.models.phrases.Phrase\n",
    "        bigram model and trigram model\n",
    "    '''\n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(text_data, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[text_data], threshold=100)  \n",
    "\n",
    "    bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_model = gensim.models.phrases.Phraser(trigram)\n",
    "    \n",
    "    return bigram_model, trigram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigram_trigram_lists(text_data, bigram_model, trigram_model):\n",
    "    '''\n",
    "    Forms bigram and trigram list of lists\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    text_data:  list of tokens\n",
    "        tokens from reviews/tips text column\n",
    "    Returns\n",
    "    -------\n",
    "    bigrams: list of lists (str)\n",
    "    trigrams: list of lists (str)\n",
    "        Returns text data consisting of bigrams and trigrams\n",
    "    '''\n",
    "    bigrams = []\n",
    "    trigrams = []\n",
    "    \n",
    "    for text in text_data:\n",
    "        bigrams.append(bigram_model[text])\n",
    "        trigrams.append(trigram_model[bigram_model[text]])\n",
    "    \n",
    "    return bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dominant_topics(lda_model, row, df):\n",
    "    \"\"\"\n",
    "    Identifies dominant topic and its percentage contribution in each document.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lda_model: gensim.models.ldamulticore.LdaMulticore\n",
    "        LDA model that holds all topics\n",
    "    row: enumerate object\n",
    "        list of tuples consisting of topic number and its contribution to the document\n",
    "    df : pandas.DataFrame\n",
    "        Reviews or Tips DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df: pandas.DataFrame\n",
    "        Dataframe with dominant topic number, percentage contribution, and topic keywords\n",
    "    \"\"\"\n",
    "    topics_df = pd.DataFrame() # Init dataframe\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    for i, row_list in row: # Get main topic in each doc\n",
    "        \n",
    "        row = row_list[0] if lda_model.per_word_topics else row_list\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True) # Arranging topics based on its contribution\n",
    "        \n",
    "        topic_num = row[0][0]\n",
    "        prop_topic = row[0][1]\n",
    "        words = lda_model.show_topic(topic_num)\n",
    "        \n",
    "        topic_keywords = ', '.join([word for word, prop in words])\n",
    "        data = pd.Series([int(topic_num), round(prop_topic,3), topic_keywords])\n",
    "        topics_df = topics_df.append(data, ignore_index=True)\n",
    "\n",
    "    topics_df.columns = ['dominant_topic', 'percent_contribution', 'topic_keywords']\n",
    "    topics_df = pd.concat([topics_df, df['clean_text']], axis=1)\n",
    "    return topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(id2word, tfidf_corpus, text_data, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    id2word : Gensim dictionary\n",
    "    tfidf_corpus : Gensim corpus\n",
    "    text_data : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    lda_tuning_result_df: pandas.DataFrame consists of topics, alpha, beta, and coherence values\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    tfidf_model_list = []\n",
    "    \n",
    "    # Alpha parameter\n",
    "    alpha = list(np.arange(0.01, 1, 0.3))\n",
    "    alpha.append('symmetric')\n",
    "    alpha.append('asymmetric')\n",
    "\n",
    "    # Beta parameter\n",
    "    beta = list(np.arange(0.01, 1, 0.3))\n",
    "    beta.append('symmetric')\n",
    "    \n",
    "    # Gets model info\n",
    "    model_results = {'Topics': [],\n",
    "                     'Alpha': [],\n",
    "                     'Beta': [],\n",
    "                     'Coherence': []}\n",
    "    \n",
    "    for num_topics in range(start, limit, step):\n",
    "        \n",
    "        # iterate through alpha values\n",
    "        for a in alpha:\n",
    "            \n",
    "            # iterare through beta values\n",
    "            for b in beta:\n",
    "                tfidf_lda_model = LdaMulticore(tfidf_corpus, # stream of document vectors \n",
    "                                       id2word=id2word, # mapping from word IDs to words\n",
    "                                       num_topics=num_topics,  # number of requested latent topics to be extracted from the training corpu\n",
    "                                       chunksize=100, # number of docs to be used in each training chunk\n",
    "                                       alpha=a, \n",
    "                                       eta=b, \n",
    "                                       random_state=42, # to ensure same result\n",
    "                                       eval_every=None, # don't evaluate model perplexity - takes too long\n",
    "                                       passes=2, # number of passes through the corpus during training\n",
    "                                       workers=4)\n",
    "        \n",
    "                tfidf_model_list.append(tfidf_lda_model)\n",
    "                coherencemodel = CoherenceModel(model=tfidf_lda_model, texts=text_data, dictionary=id2word, coherence='c_v')\n",
    "                coherence_value = coherencemodel.get_coherence()\n",
    "                coherence_values.append(coherence_value)\n",
    "                \n",
    "                # Save the model results\n",
    "                model_results['Topics'].append(num_topics)\n",
    "                model_results['Alpha'].append(a)\n",
    "                model_results['Beta'].append(b)\n",
    "                model_results['Coherence'].append(coherence_value)\n",
    "                \n",
    "    lda_tuning_result_df = pd.DataFrame(model_results)\n",
    "                \n",
    "    return tfidf_model_list, coherence_values, lda_tuning_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_doc_word_counts(df, doc_lens, ax_count, row_count, start_index=0):\n",
    "    '''\n",
    "    Plots distribution of document word counts.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Yelp's review or tip dataframe\n",
    "    doc_lens: list\n",
    "        list of word count per document\n",
    "    ax_count: int\n",
    "        # of axes per row\n",
    "    row_count: int\n",
    "        # of rows\n",
    "    start_index: int\n",
    "        starting index during loop\n",
    "    '''\n",
    "    colors = [color for name, color in mcolors.XKCD_COLORS.items()]\n",
    "    random.shuffle(colors)\n",
    "\n",
    "    #dpi = dots per inches - sharex/sharey - controls sharing of properties among x and y axes among all subplots \n",
    "    fig, axes = plt.subplots(ax_count, row_count, figsize=(15,10), dpi=100, sharex=True, sharey=True)\n",
    "\n",
    "    for i, ax in enumerate(axes.flatten(), start_index): # flatten - flattens axes group into individual ax\n",
    "\n",
    "        # Get individual topic dataframe\n",
    "        sub_df = df.loc[df['dominant_topic'] == i, :]\n",
    "\n",
    "        # Get word counts per document\n",
    "        doc_lens = [len(text.split()) for text in sub_df['clean_text']]\n",
    "\n",
    "        # Create histogram with 300 bins and specified random color\n",
    "        ax.hist(doc_lens, bins=300, color=colors[i])\n",
    "\n",
    "        # Set ticks on y-axis with set label color with specified random color\n",
    "        ax.tick_params(axis='y', labelcolor=colors[i], color=colors[i])\n",
    "\n",
    "        # Set kdeplot\n",
    "        sns.kdeplot(doc_lens, color=\"black\", shade=False, ax=ax.twinx())\n",
    "\n",
    "        ax.set(xlim=(0, 300), xlabel='Document Word Count')\n",
    "        ax.set_ylabel('Number of Documents', color=colors[i])\n",
    "        ax.set_title(f'Topic: {i}', fontdict=dict(size=15, color=colors[i]))\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.90)\n",
    "    plt.xticks(np.linspace(0,300,10))\n",
    "    fig.suptitle('Distribution of Document Word Counts by Dominant Topic', fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcounts(df, ax_count, row_count, start_index=0):\n",
    "    '''\n",
    "    Plot Word Count and Weights of Topic Keywords.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Dataframe consisting of topic keyword, topic id, weight (importance), and word counts\n",
    "    ax_count: int\n",
    "        # of axes per row\n",
    "    row_count: int\n",
    "        # of rows\n",
    "    start_index: int\n",
    "        starting index during loop\n",
    "    '''\n",
    "    # Get random colors\n",
    "    colors = [color for name, color in mcolors.XKCD_COLORS.items()]\n",
    "    random.shuffle(colors)\n",
    "    \n",
    "    fig, axes = plt.subplots(ax_count, row_count, figsize=(12,7), sharey=True, dpi=100)\n",
    "\n",
    "    for i, ax in enumerate(axes.flatten(), start_index):\n",
    "        \n",
    "        # Create bar graph - word count\n",
    "        ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=colors[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "        ax_twin = ax.twinx() # share bar graph\n",
    "        \n",
    "        # Weightage (importance)\n",
    "        ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=colors[i], width=0.2, label='Weights')\n",
    "        ax.set_ylabel('Word Count', color=colors[i])\n",
    "        \n",
    "        # Set y-axis view limits\n",
    "        ax.set_ylim(0, 5000000);\n",
    "        ax_twin.set_ylim(0, 0.030); \n",
    "\n",
    "        # Set title\n",
    "        ax.set_title(f'Topic: {i}', color=colors[i], fontsize=15)\n",
    "        ax.tick_params(axis='y', left=False)\n",
    "        \n",
    "        # Tilt x-labels 30 deg\n",
    "        ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment='right')\n",
    "        \n",
    "        # Set legend \n",
    "        ax.legend(loc='upper left'); \n",
    "        ax_twin.legend(loc='upper right')\n",
    "\n",
    "    fig.tight_layout(w_pad=2)    \n",
    "    fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=20, y=1.05)    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud(df, lda_model, ax_count, row_count, start_index=0):\n",
    "    '''\n",
    "    Shows top ten words by weight per topic.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Yelp's review or tip dataframe\n",
    "    lda_model: gensim.models.ldamulticore.LdaMulticore\n",
    "        LDA model that holds all topics\n",
    "    ax_count: int\n",
    "        # of axes per row\n",
    "    row_count: int\n",
    "        # of rows\n",
    "    start_index: int\n",
    "        starting index during loop\n",
    "    '''\n",
    "    colors = [color for name, color in mcolors.XKCD_COLORS.items()]\n",
    "    random.shuffle(colors)\n",
    "    \n",
    "    # instantiate word cloud\n",
    "    word_cloud = WordCloud(stopwords=stop_words,# using spacy's stopwords\n",
    "                           width=2500, # width of canvas\n",
    "                           height=1800, # height of canvas\n",
    "                           max_words=10, # Shows top 10 words by weight\n",
    "                           color_func=lambda *args, **kwargs: colors[i], # Sets color\n",
    "                           prefer_horizontal=1.0) # Horizontal fitting\n",
    "    \n",
    "    # formatted=false - returns 2 tuples of (word, probability)\n",
    "    # num_topics=-1 - shows all topics\n",
    "    topics = lda_model.show_topics(formatted=False, num_topics=-1) \n",
    "    \n",
    "    fig, axes = plt.subplots(ax_count, row_count, figsize=(15,10), sharex=True, sharey=True)\n",
    "\n",
    "    for i, ax in enumerate(axes.flatten(), start_index):\n",
    "        fig.add_subplot(ax)\n",
    "        topic_words = dict(topics[i][1])\n",
    "        word_cloud.generate_from_frequencies(topic_words, max_font_size=300)  # Creates word cloud based on frequencies\n",
    "        plt.gca().imshow(word_cloud) # imshow - display data as image\n",
    "        plt.gca().set_title(f'Topic {i}', fontdict=dict(size=15)) # setting title\n",
    "        plt.gca().axis('off') # removing axis\n",
    "\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)# remove spacing between axes\n",
    "    plt.axis('off')\n",
    "    plt.margins(x=0, y=0) # remove margins\n",
    "    plt.tight_layout() # automatically adjusts padding between and around subplots\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud(df, cuisine, col):\n",
    "    '''\n",
    "    Shows top ten words by weight per topic.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Dataframe with text and menu values\n",
    "    cuisine: str\n",
    "        cuisine type (ex: japanese, american, etc.)\n",
    "    col: str\n",
    "        column name\n",
    "    Returns:\n",
    "    -------\n",
    "    WordCloud: wordcloud.wordcloud.WordCloud\n",
    "        WordCloud object with 50 common words\n",
    "    '''\n",
    "    colors = [color for name, color in mcolors.XKCD_COLORS.items()]\n",
    "    random.shuffle(colors)\n",
    "    \n",
    "    vec = TfidfVectorizer(stop_words='english') # Instantiation\n",
    "    vecs = vec.fit_transform(df[df[cuisine] == 1][col]) # Learns vocularly and returns vectors based on tf-idf\n",
    "    \n",
    "    feature_names = vec.get_feature_names() # Gets text value\n",
    "    dense = vecs.todense() # Converts sparse matrix to dense matrix\n",
    "    \n",
    "    df = pd.DataFrame(dense.tolist(), columns=feature_names) # Create dataframe\n",
    "    transposed_series = df.T.sum(axis=1) # Switch row index as column index - vice versa and get column sums\n",
    "    \n",
    "    return WordCloud(max_words=25, \n",
    "                     color_func=lambda *args, **kwargs: colors[0], # Sets color\n",
    "                    ).generate_from_frequencies(transposed_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dominant_topics_and_perc(lda_model, tfidf_corpus, start_index, end_index):\n",
    "    '''\n",
    "    Gets dominant topics and topics percentage list.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lda_model: gensim.models.ldamulticore.LdaMulticore\n",
    "        LDA model that holds all topics\n",
    "    corpus: gensim.interfaces.TransformedCorpus\n",
    "        tfidf corpus\n",
    "    start_index: int\n",
    "        Starting index\n",
    "    end_index: int\n",
    "        Ending index\n",
    "    Returns\n",
    "    -------\n",
    "    dominant_topics: list of dominant topics\n",
    "        Topic id and dominant topic words\n",
    "    topic_percentages: list of topic percentages\n",
    "        Topic id and topic contribution percentage \n",
    "    '''\n",
    "    corpus_sel = tfidf_corpus[start_index:end_index]\n",
    "    dominant_topics = []\n",
    "    topic_percentages = []\n",
    "    \n",
    "    for i, corp in enumerate(corpus_sel): # [(0, 0.09631027287834815), (1, 0.12703629397704447), ...]\n",
    "        topic_percs, wordid_topics = lda_model[corp]\n",
    "        dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse=True)[0][0]\n",
    "        dominant_topics.append((i, dominant_topic))\n",
    "        topic_percentages.append(topic_percs)\n",
    "        \n",
    "    return(dominant_topics, topic_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_top_three_words_df(dominant_topics):\n",
    "    '''\n",
    "    Creates top three words per topic dataframe\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lda_model: list of dominant topics\n",
    "        Topic id and dominant topic words\n",
    "    Returns\n",
    "    -------\n",
    "    dominant_topics: list of dominant topics\n",
    "        Topic id and dominant topic words\n",
    "    '''\n",
    "    # Distribution of Dominant Topics in Each Document\n",
    "    df = pd.DataFrame(dominant_topics, columns=['Document_Id', 'Dominant_Topic'])\n",
    "    dominant_topic_in_each_doc = df.groupby('Dominant_Topic').size()\n",
    "    df_dominant_topic_in_each_doc = dominant_topic_in_each_doc.to_frame(name='count').reset_index()\n",
    "\n",
    "    # Total Topic Distribution by actual weight\n",
    "    topic_weightage_by_doc = pd.DataFrame([dict(t) for t in topic_percentages])\n",
    "    df_topic_weightage_by_doc = topic_weightage_by_doc.sum().to_frame(name='count').reset_index()\n",
    "\n",
    "    # Top 3 Keywords for each Topic\n",
    "    topic_top3words = [(i, topic) for i, topics in lda_model.show_topics(formatted=False) \n",
    "                                     for j, (topic, wt) in enumerate(topics) if j < 3]\n",
    "\n",
    "    df_top3words_stacked = pd.DataFrame(topic_top3words, columns=['topic_id', 'words'])\n",
    "    df_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\n",
    "    df_top3words.reset_index(level=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(x, index, n_gram):\n",
    "    '''\n",
    "    Returns top n words in unigram, bigram, or trigram.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: pandas.Series\n",
    "        Text values\n",
    "    index: int\n",
    "        Index which determines how many top n keywords we want\n",
    "    n_gram: int\n",
    "        Determines we want one or two-words or three-words \n",
    "    Returns\n",
    "    -------\n",
    "    words_freq: list of tuples\n",
    "         List of tuples with text as key and value as count\n",
    "    '''\n",
    "    vec = CountVectorizer(ngram_range=(n_gram, n_gram), stop_words='english').fit(x)\n",
    "    bow = vec.transform(x)\n",
    "    sum_words = bow.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0,idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(a, b):\n",
    "    '''\n",
    "    Defines cosine similarity distance\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a: list of dummy business attributes values (binary)\n",
    "    b: list of business attributes values from business dataframe(binary)\n",
    "    Returns\n",
    "    -------\n",
    "    sim: float\n",
    "        cosine similarity distance\n",
    "    '''\n",
    "    nom = np.sum(np.multiply(a, b))\n",
    "    denom = np.sqrt(np.sum(np.square(a))) * np.sqrt(np.sum(np.square(b)))\n",
    "    sim = nom / denom\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d2v_ranking_ir(query, model, df, model_type):\n",
    "    '''\n",
    "    Generates query result dataframe based on ranking information retrieval.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query: str\n",
    "        Text value\n",
    "    model: gensim.models.doc2vec.Doc2Vec\n",
    "        Doc2Vec model\n",
    "    df: pandas.DataFrame\n",
    "        Cleaned text value dataframe consisting of restaurant information\n",
    "    model_type: str\n",
    "        text or menu type (defining which w2v model is used)\n",
    "    Returns\n",
    "    -------\n",
    "    result_df: pandas.DataFrame\n",
    "        Dataframe with restaurant name and cosine similarity score\n",
    "    '''\n",
    "    \n",
    "    # Preprocess query\n",
    "    query = expand_contractions(query) # to ensure vocab uniformity with w2v vocab\n",
    "    query = tokenize(query) # lowercase, lemmatizes, and removes stop word\n",
    "    query = re.sub(' +', ' ', query) # Remove extra whitespace\n",
    "    \n",
    "    model.random.seed(42)\n",
    "    \n",
    "    # Generate vector\n",
    "    vector = model.infer_vector(test_value.split() ,alpha=0.001 ,steps = 5)\n",
    "    \n",
    "    # Ranking documents\n",
    "    if model_type == 'text':\n",
    "        documents = df[['name', 'clean_text', 'popularity_score']].copy()\n",
    "    elif model_type == 'menu_desc':\n",
    "        documents = df[['name', 'clean_menu_desc', 'popularity_score']].copy()\n",
    "    else:\n",
    "        documents = df[['name', 'clean_menu_titles', 'popularity_score']].copy()\n",
    "    \n",
    "    # Applies cosine similarity after reshaping np array into one single nested list\n",
    "    documents['similarity'] = df['vector'].apply(lambda x: cosine_similarity(np.array(vector).reshape(1,-1), \n",
    "                                                                             np.array(x).reshape(1, -1)).item())\n",
    "    \n",
    "    # Sort by similarity score\n",
    "    documents.sort_values(by='similarity', ascending=False, inplace=True)\n",
    "    \n",
    "    return documents.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_doc_tuples(df, dfType = 'text'):\n",
    "    '''\n",
    "    Create document tuples consisting of restaurant name, popularity score, words, tagged document id, and index\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "        Text or Menu dataframes\n",
    "    dfType: str\n",
    "        text, menu_desc, or menu_title\n",
    "    Returns\n",
    "    -------\n",
    "    docs: list of tuples\n",
    "        list of Document instances (tuples)\n",
    "    '''\n",
    "    Document = namedtuple('Document', 'name words popularity_score tags index')\n",
    "    \n",
    "    text_col = 'clean_text'\n",
    "    docs = [] # Storing all named tuples\n",
    "    \n",
    "    # Get different column name if menu dataframe is given\n",
    "    if dfType == 'menu_desc':\n",
    "        text_col = 'clean_menu_desc'\n",
    "    elif dfType == 'menu_title':\n",
    "        text_col = 'clean_menu_titles'\n",
    "        \n",
    "    for i in range(df.shape[0]):\n",
    "        name = df['name'].values[i] # restaurant name\n",
    "        words = df[text_col].values[i].split() # text values\n",
    "        popularity_score = df['popularity_score'].values[i] # popularity score\n",
    "        tags = [i] # tagged doc id\n",
    "        index = i # list index\n",
    "            \n",
    "        docs.append(Document(name, words, popularity_score, tags, index))\n",
    "        \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_categories(query):\n",
    "    '''\n",
    "    Identifies keywords from query for filtering menu and text dataframes prior to doc2vec's information retrieval.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query: str\n",
    "        Text value\n",
    "    Returns\n",
    "    -------\n",
    "    names: list of str\n",
    "        list of restaurant keywords\n",
    "    '''\n",
    "    # Default dataframes\n",
    "    text_result_df = None\n",
    "    \n",
    "    query = expand_contractions(query) # to ensure vocab uniformity with d2v vocab\n",
    "    query = tokenize(query) # lowercase, lemmatizes, and removes stop word\n",
    "    query = re.sub(' +', ' ', query) # Remove extra whitespace\n",
    "\n",
    "    text_list = list(set(query.split()))\n",
    "    dfs = []\n",
    "    names = []\n",
    "    \n",
    "    # Only allow search when there is search term\n",
    "    if len(text_list) > 0:\n",
    "        \n",
    "        # When there is keywors that matches\n",
    "        for text in text_list:\n",
    "\n",
    "            # Check if text is a key of categories dictionary\n",
    "            if text in categories_dict:\n",
    "                # Append to dfs list\n",
    "                dfs.append(categories_dict[text])\n",
    "\n",
    "        # When search term(s) matches one of restaurants' categories\n",
    "        if len(dfs) > 0:\n",
    "\n",
    "            result_df = None\n",
    "\n",
    "            # Concat all dataframes\n",
    "            if len(dfs) > 1:\n",
    "                result_df = pd.concat(dfs)\n",
    "            else:\n",
    "                result_df = dfs[0]\n",
    "                \n",
    "            # Drop duplicates\n",
    "            result_df = result_df.drop_duplicates(keep='first')\n",
    "            result_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            names = result_df['name'].values # Get all restaurant names\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_list(query, model):\n",
    "    '''\n",
    "    Returns restaurant names that corresponds with query (ex: 'chinese' would return chinese restaurant names)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query: str\n",
    "        Query\n",
    "    model: gensim.models.doc2vec.Doc2Vec\n",
    "        Text or menn trained doc2vec model\n",
    "    Returns\n",
    "    -------\n",
    "    docs: list of tuples\n",
    "        list of Document instances (tuples)\n",
    "    '''\n",
    "    # Clean text value\n",
    "    query = expand_contractions(query) # to ensure vocab uniformity with d2v vocab\n",
    "    query = tokenize(query) # lowercase, lemmatizes, and removes stop word\n",
    "    query = re.sub(' +', ' ', query) # Remove extra whitespace\n",
    "\n",
    "    # Get restaurant names that share same characteristic as query\n",
    "    restaurant_names = search_categories(query)\n",
    "    \n",
    "    if len(restaurant_names) > 0:\n",
    "        restaurant_names = restaurant_names.tolist()\n",
    "    \n",
    "    # Get similar words\n",
    "    try:\n",
    "        text_list = list(set(query.split()))\n",
    "        \n",
    "        for text in text_list:\n",
    "            similar_words = model.wv.most_similar_cosmul(positive=[text])\n",
    "\n",
    "            # Search for more restaurant names based on similar words\n",
    "            for index_sim in similar_words:\n",
    "\n",
    "                sim_word = index_sim[0] # restaurant name\n",
    "                names = search_categories(sim_word) \n",
    "\n",
    "                if len(names) > 0:\n",
    "\n",
    "                    for name in names:\n",
    "                        restaurant_names.append(name)\n",
    "    except:\n",
    "        print('Similar words not found')\n",
    "        \n",
    "    return list(set(restaurant_names));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_docs(query, model, df, text_col):\n",
    "    '''\n",
    "    Creates top 10 result from user search input\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query: str\n",
    "        Query\n",
    "    model: gensim.models.doc2vec.Doc2Vec\n",
    "        Text or menn trained doc2vec model\n",
    "    df: pandas.DataFrame\n",
    "        Text or Menu dataframe\n",
    "    text_col: str\n",
    "        Text column name\n",
    "    Returns\n",
    "    -------\n",
    "    result: pandas.DataFrame\n",
    "        Top 10 results\n",
    "    '''\n",
    "    # Ensure text column is a str datatype\n",
    "    df[text_col] = df[text_col].astype(str)\n",
    "    \n",
    "    # Set random seed to ensure same results\n",
    "    model.random.seed(42)\n",
    "    \n",
    "    # Populate vector\n",
    "    vector = model.infer_vector(query.split())\n",
    "    \n",
    "    # Store all relevant dataframes created through search methods\n",
    "    dfs = []\n",
    "    \n",
    "    for text in query.split():\n",
    "        \n",
    "        # Gather restaurant names if any\n",
    "        restaurant_names = get_similar_list(text, model)\n",
    "\n",
    "        # Filter list\n",
    "        if len(restaurant_names) > 0:\n",
    "            \n",
    "            sim_df = df.copy()\n",
    "            \n",
    "            sim_df = sim_df[sim_df['name'].isin(restaurant_names)]\n",
    "\n",
    "            # Remove duplicates \n",
    "            #df.drop_duplicates(subset=['name'], keep='first', inplace=True)\n",
    "\n",
    "            #df['vector'] = df[text_col].str.split().apply(model.infer_vector).values\n",
    "\n",
    "            # Applies cosine similarity after reshaping np array into one single nested list\n",
    "            sim_df['similarity_score'] = sim_df['vector'].apply(lambda x: cosine_similarity(np.array(vector).reshape(1,-1), \n",
    "                                                                             np.array(x).reshape(1, -1)).item()) \n",
    "            dfs.append(sim_df)\n",
    "        else:\n",
    "            \n",
    "            copied_df = df.copy()\n",
    "            \n",
    "            # Populate top 50 restaurants that are similar to query vector\n",
    "            similar_index = model.docvecs.most_similar([vector], topn=50)\n",
    "\n",
    "            # Get all the index and similarity scores\n",
    "            row_indices = []\n",
    "            sim_scores = []\n",
    "\n",
    "            # Create new column\n",
    "            for index_sim in similar_index:\n",
    "                row_indices.append(index_sim[0])\n",
    "                sim_scores.append(index_sim[1])\n",
    "\n",
    "            # Create new column (similarity_score)\n",
    "            copied_df['similarity_score'] = 0\n",
    "\n",
    "            # Set similarity score\n",
    "            copied_df['similarity_score'].iloc[row_indices] = sim_scores\n",
    "            copied_df = copied_df.iloc[row_indices]\n",
    "\n",
    "            dfs.append(copied_df)\n",
    "    \n",
    "    # Combine dataframes\n",
    "    result_df = pd.concat(dfs)\n",
    "\n",
    "    # Getting required columns\n",
    "    result_df = result_df[['name', text_col, 'popularity_score', 'similarity_score']]\n",
    "    \n",
    "    # Sort by similarity score\n",
    "    result_df.sort_values(by='similarity_score', ascending=False, inplace=True)\n",
    "    \n",
    "    # Remove duplicates \n",
    "    result_df.drop_duplicates(subset=['name'], keep='first', inplace=True)\n",
    "    \n",
    "    # Rename text column to 'text' for uniformity\n",
    "    result_df.rename(columns={text_col: 'text'}, inplace=True)\n",
    "    \n",
    "    return result_df.reset_index(drop=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, models, dfs):\n",
    "    '''\n",
    "    Consolidates three Doc2Vec results and populates top 10 restaurant list based on given query.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query: str\n",
    "        Query\n",
    "    models: dict - gensim.models.doc2vec.Doc2Vec\n",
    "        Contains text, menu description, and menu title's doc2vec models\n",
    "    dfs: dict - pandas.DataFrame\n",
    "        Contains text, menu description, and menu title's doc2vec models\n",
    "    Returns\n",
    "    -------\n",
    "    result: pandas.DataFrame\n",
    "        Top 10 results after consolidating 3 Doc2Vec results\n",
    "    '''\n",
    "    # Clean text value\n",
    "    query = expand_contractions(query) # to ensure vocab uniformity with d2v vocab\n",
    "    query = tokenize(query) # lowercase, lemmatizes, and removes stop word\n",
    "    query = re.sub(' +', ' ', query) # Remove extra whitespace\n",
    "    \n",
    "    # Find similar documents\n",
    "    result_text_df = find_similar_docs(query, models['text'], dfs['text'], 'clean_text')\n",
    "    result_menu_desc_df = find_similar_docs(query, models['desc'], dfs['desc'], 'clean_menu_desc')\n",
    "    result_menu_title_df = find_similar_docs(query, models['title'], dfs['title'], 'clean_menu_titles')\n",
    "    \n",
    "    # Consolidate results\n",
    "    result_df = pd.concat([result_menu_desc_df, result_menu_title_df, result_text_df])\n",
    "    \n",
    "    # Get rows based on duplicate status\n",
    "    final_result_df = result_df[result_df.duplicated(subset=['name'])]\n",
    "    \n",
    "    # When there is duplicate - gather selected restaurant names\n",
    "    duplicate_count = len(final_result_df)\n",
    "    \n",
    "    if duplicate_count > 0 and duplicate_count < 10:\n",
    "        # Determine how many more values are needed to accumulate up to 10 results\n",
    "        index = 10 - duplicate_count\n",
    "        \n",
    "        # Get rows with unique restaurant names\n",
    "        unique_df = result_df[~result_df['name'].isin(final_result_df['name'].values)]\n",
    "        \n",
    "        # Sort by similarity score\n",
    "        unique_df.sort_values(by='similarity_score', ascending=False, inplace=True)\n",
    "\n",
    "        # Get row(s)\n",
    "        row_df = unique_df[:index]\n",
    "        \n",
    "        # Concat\n",
    "        final_result_df = pd.concat([final_result_df, row_df])\n",
    "        \n",
    "        # Sort by similarity score\n",
    "        final_result_df.sort_values(by='similarity_score', ascending=False, inplace=True)\n",
    "        \n",
    "        # Reset index\n",
    "        return final_result_df.reset_index(drop=True)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # Sort by similarity score\n",
    "        result_df.sort_values(by='similarity_score', ascending=False, inplace=True)\n",
    "        \n",
    "        # Reset index\n",
    "        return result_df.reset_index(drop=True)[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
