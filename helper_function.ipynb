{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "from gensim.models import CoherenceModel, TfidfModel, LdaMulticore, Word2Vec, KeyedVectors\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import random \n",
    "import re\n",
    "\n",
    "# Ignore warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy for lemmatization and stop words\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Add additional stop words to Spacy\n",
    "custom_stop_words = ['good', 'great', 'love', 'eat', 'try', 'amazing', 'come', 'food', 'place', 'order', \n",
    "                     'service', 'time', 'definitely', 'outstanding', 'restaurant', 'like', 'get', 'nice', \n",
    "                     'go', 'excellent', 'serve', 'sauce', 'bad', 'price']\n",
    "\n",
    "for word in custom_stop_words:\n",
    "    stop_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTION_DICT = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_matrix_df(df, vocab=None):\n",
    "    '''\n",
    "    Creates tf-idf matrix dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Yelp's review or tip dataframe\n",
    "    vocab (optional): list\n",
    "        list of words used for defining vocabular in CountVectorizer\n",
    "    Returns\n",
    "    -------\n",
    "    dtm_tfidf_df : pandas.DataFrame\n",
    "        tf-idf dataframe with Yelp's business id as row index\n",
    "    '''\n",
    "    \n",
    "    # Instantiation\n",
    "    if vocab != None:\n",
    "        vectorizer = CountVectorizer(min_df=.01, # min_df - ignore terms that appear less than 1% of the documents\n",
    "                                     max_df=0.5, # max_df - ignore terms that appear in more than 65% of documents\n",
    "                                     vocabulary=vocab) # ngram_range - consider unigrams, bigrams, trigrams and so forth\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(min_df=.1, # min_df - ignore terms that appear less than 1% of the documents\n",
    "                                     max_df=0.5, # max_df - ignore terms that appear in more than 65% of documents\n",
    "                                     ngram_range=(1,3)) # ngram_range - consider unigrams, bigrams, trigrams and so forth\n",
    "    \n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    \n",
    "    X = vectorizer.fit_transform(df['item_list'])\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "\n",
    "    # Create document term matrix dataframe\n",
    "    dtm_df = pd.DataFrame(X.toarray(), index = df['cuisine'], columns = vocab)\n",
    "\n",
    "    # Create tfidf matrix\n",
    "    X_tfidf = tfidf_transformer.fit_transform(dtm_df)\n",
    "    dtm_tfidf_df = pd.DataFrame(X_tfidf.toarray(), index = df['cuisine'], columns = vocab)\n",
    "    \n",
    "    return dtm_tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_menu_links(location_list):\n",
    "    '''\n",
    "    Collects menu links from allmenus.com - which is collected using state.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    location_list: list of objects\n",
    "        List of objects containing state and cities per object.\n",
    "    Returns\n",
    "    -------\n",
    "    links: list of str\n",
    "        allmenus' url links\n",
    "    rest_titles: list of str\n",
    "        Restaurant name/title\n",
    "    '''\n",
    "    cuisines = ['greek', 'south-american', 'filipino', 'indian', 'jamaican', 'spanish', 'italian', 'mexican', \n",
    "                'chinese', 'british-traditional', 'thai', 'vietnamese', 'brazilian', 'french', 'japanese', 'irish', \n",
    "                'korean', 'moroccan', 'russian']\n",
    "    \n",
    "    links = [] # Note: list for restaurant links\n",
    "    cuisine_list = []\n",
    "    rest_titles = []\n",
    "    \n",
    "    for location in location_list:\n",
    "        \n",
    "        state = location['state']\n",
    "        cities = location['cities']\n",
    "\n",
    "        for city in cities:\n",
    "\n",
    "            # Get restaurants based on cuisines\n",
    "            for cuisine in cuisines:\n",
    "\n",
    "                page = requests.get(f'https://www.allmenus.com/{state}/{city}/-/{cuisine}/')\n",
    "                soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "                # Get all restaurant titles and cuisine list\n",
    "                h4_elems = soup.find_all('h4', class_='name')\n",
    "                p_elems = soup.find_all('p', class_='cousine-list')\n",
    "\n",
    "                i = 0 # iterator for p tags in getting cuisine list\n",
    "\n",
    "                # Loop through all h4 tags for child a tag's hrefs\n",
    "                for elem in h4_elems: \n",
    "\n",
    "                    a_elem = elem.find_all('a')[0] # Get child a tag\n",
    "                    cuisine_type = p_elems[i].getText() # Get cuisine type\n",
    "\n",
    "                    link = a_elem.get('href') # Get href\n",
    "                    links.append(f'{link}%{cuisine_type}')\n",
    "                    rest_titles.append(a_elem.getText())\n",
    "                    i += 1\n",
    "\n",
    "    # Remove duplicate restaurants\n",
    "    return (links, rest_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_menu_df(links, rest_titles):\n",
    "    '''\n",
    "    Builds menu dataframe\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    links: list of str\n",
    "        allmenus' url links to restaurant menu list\n",
    "    rest_titles: list of str\n",
    "        Restaurant name/title\n",
    "    Returns\n",
    "    -------\n",
    "    menu_df: pandas.DataFrame\n",
    "        DataFrame with restaurant name, menu title, menu description, and categories columns\n",
    "    '''\n",
    "    # Collect menu data per restaurant\n",
    "    ids = []\n",
    "    rest_names = []\n",
    "    categories = []\n",
    "    menu_titles = []\n",
    "    menu_desc = []\n",
    "\n",
    "    j = 0 # Iterator for retrieving cuisine list\n",
    "    \n",
    "    # Looping through each restaurant\n",
    "    for link in links:\n",
    "\n",
    "        # Retrieve city\n",
    "        state = link.split('/')[1]\n",
    "        start = f'/{state}/'\n",
    "        end = '/'\n",
    "        city = link.split(start)[1].split(end)[0]\n",
    "\n",
    "        # Get restauarnt id\n",
    "        start = f'/{state}/{city}/'\n",
    "        end = '-'\n",
    "        rest_id = int(link.split(start)[1].split(end)[0])\n",
    "\n",
    "        # Get food categories - 'asian, american, indian etc.'\n",
    "        idx = link.find('%')\n",
    "        category = link[idx + 1:]\n",
    "\n",
    "        # Remove category at the end of the link\n",
    "        link = link[:idx] \n",
    "\n",
    "        # Get restaurant page\n",
    "        page = requests.get(f'https://www.allmenus.com{link}')\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        # Get restaurant name\n",
    "        rest_name = rest_titles[j]\n",
    "        j += 1\n",
    "\n",
    "        # Get menu items and its descriptions\n",
    "        item_titles = soup.find_all('span', class_='item-title')\n",
    "        item_desc = soup.find_all('p', class_='description')\n",
    "        i = 0 # iterator for restaurant ids\n",
    "\n",
    "        # Loop menu titles\n",
    "        for item_title in item_titles:\n",
    "\n",
    "            # Add restaurant id, name, categories, menu title, menu description\n",
    "            ids.append(rest_id)\n",
    "            rest_names.append(rest_name)\n",
    "            categories.append(category)\n",
    "            menu_titles.append(item_title.getText())\n",
    "            \n",
    "            if len(item_desc) != 0:\n",
    "                menu_desc.append(item_desc[i].getText())\n",
    "            i +=1\n",
    "    \n",
    "    # Create menus dataframe with collected data\n",
    "    d = {'id': ids, 'name': rest_names, 'menu_titles': menu_titles, 'menu_desc': menu_desc, 'categories': categories}\n",
    "    menu_df = pd.DataFrame(data=d)\n",
    "    \n",
    "    return menu_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_dict=CONTRACTION_DICT):\n",
    "    \"\"\"\n",
    "    Expands contractions. For example, \"y'all can't\" => \"you cannot\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Text data\n",
    "    Returns\n",
    "    -------\n",
    "    expanded_text: str\n",
    "        Returns expanded text\n",
    "    \"\"\"\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_dict.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_dict.get(match)\\\n",
    "                                if contraction_dict.get(match)\\\n",
    "                                else contraction_dict.get(match.lower())\n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "    \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    \n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Removes special characters\\whitespaces, lowercases, tokenize, filter based on stop words \n",
    "    and lemmatize.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Yelp review or tips text data\n",
    "    Returns\n",
    "    -------\n",
    "    clean_text: str\n",
    "        Returns tokenized text\n",
    "    \"\"\"\n",
    "    tokens = nlp(text, disable=['parser', 'ner'])\n",
    "    #pos_tags = ['NOUN', 'VERB', 'ADJ', 'ADV'] # allowed parts of speech\n",
    "    i = 0 # iterator to store lemmatized and clean text back to dataframe\n",
    "    clean_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        \n",
    "        if (not token.is_punct # no punctuation\n",
    "            and not token.is_space # no whitespace\n",
    "            and token.is_alpha): # include alphabets\n",
    "            #and token.pos_ in pos_tags): # include noun, verb, adjectives, adverbs\n",
    "            \n",
    "            if str(token) not in stop_words: # no stop words\n",
    "\n",
    "                word = token.lemma_.strip().lower() # lemmatize, whitespace and lowercase\n",
    "                clean_tokens.append(word)\n",
    "                \n",
    "    clean_text = ' '.join(clean_tokens) # re-create text from clean tokens\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrams_trigrams_list(text):\n",
    "    '''\n",
    "    Builds bigram and trigram models and returns list of bigrams and trigrams list\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    text:  pandas.Series\n",
    "        Text values from Consolidated Dataframe (reviews, tips, menu ingredient, etc.)\n",
    "    Returns\n",
    "    -------\n",
    "    bigram and trigram: list (str)\n",
    "        list of bigrams and trigrams\n",
    "    '''\n",
    "    text_data = [] # Storing each tokens\n",
    "    text.apply(lambda text: text_data.append(text.split(' '))); # Split by individual words\n",
    "    \n",
    "    # Build bigram and trigram models\n",
    "    bigram_model, trigram_model = build_bigram_trigram_models(text_data)\n",
    "    \n",
    "    # Build bigrams and trigrams list\n",
    "    bigrams_list, trigrams_list = build_bigram_trigram_lists(text_data, bigram_model, trigram_model)\n",
    "    \n",
    "    return bigrams_list, trigrams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigram_trigram_models(text_data):\n",
    "    '''\n",
    "    Builds bigram and trigram models\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    text_data:  list of tokens\n",
    "        tokens from reviews/tips text column\n",
    "    Returns\n",
    "    -------\n",
    "    bigram_model and trigram_model: gensim.models.phrases.Phrase\n",
    "        bigram model and trigram model\n",
    "    '''\n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(text_data, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[text_data], threshold=100)  \n",
    "\n",
    "    bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_model = gensim.models.phrases.Phraser(trigram)\n",
    "    \n",
    "    return bigram_model, trigram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigram_trigram_lists(text_data, bigram_model, trigram_model):\n",
    "    '''\n",
    "    Forms bigram and trigram list of lists\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    text_data:  list of tokens\n",
    "        tokens from reviews/tips text column\n",
    "    Returns\n",
    "    -------\n",
    "    bigrams: list of lists (str)\n",
    "    trigrams: list of lists (str)\n",
    "        Returns text data consisting of bigrams and trigrams\n",
    "    '''\n",
    "    bigrams = []\n",
    "    trigrams = []\n",
    "    \n",
    "    for text in text_data:\n",
    "        bigrams.append(bigram_model[text])\n",
    "        trigrams.append(trigram_model[bigram_model[text]])\n",
    "    \n",
    "    return bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dominant_topics(lda_model, row, df):\n",
    "    \"\"\"\n",
    "    Identifies dominant topic and its percentage contribution in each document.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lda_model: gensim.models.ldamulticore.LdaMulticore\n",
    "        LDA model that holds all topics\n",
    "    row: enumerate object\n",
    "        list of tuples consisting of topic number and its contribution to the document\n",
    "    df : pandas.DataFrame\n",
    "        Reviews or Tips DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df: pandas.DataFrame\n",
    "        Dataframe with dominant topic number, percentage contribution, and topic keywords\n",
    "    \"\"\"\n",
    "    topics_df = pd.DataFrame() # Init dataframe\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    for i, row_list in row: # Get main topic in each doc\n",
    "        \n",
    "        row = row_list[0] if lda_model.per_word_topics else row_list\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True) # Arranging topics based on its contribution\n",
    "        \n",
    "        topic_num = row[0][0]\n",
    "        prop_topic = row[0][1]\n",
    "        words = lda_model.show_topic(topic_num)\n",
    "        \n",
    "        topic_keywords = ', '.join([word for word, prop in words])\n",
    "        data = pd.Series([int(topic_num), round(prop_topic,3), topic_keywords])\n",
    "        topics_df = topics_df.append(data, ignore_index=True)\n",
    "\n",
    "    topics_df.columns = ['dominant_topic', 'percent_contribution', 'topic_keywords']\n",
    "    topics_df = pd.concat([topics_df, df['clean_text']], axis=1)\n",
    "    return topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(id2word, tfidf_corpus, text_data, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    id2word : Gensim dictionary\n",
    "    tfidf_corpus : Gensim corpus\n",
    "    text_data : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    lda_tuning_result_df: pandas.DataFrame consists of topics, alpha, beta, and coherence values\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    tfidf_model_list = []\n",
    "    \n",
    "    # Alpha parameter\n",
    "    alpha = list(np.arange(0.01, 1, 0.3))\n",
    "    alpha.append('symmetric')\n",
    "    alpha.append('asymmetric')\n",
    "\n",
    "    # Beta parameter\n",
    "    beta = list(np.arange(0.01, 1, 0.3))\n",
    "    beta.append('symmetric')\n",
    "    \n",
    "    # Gets model info\n",
    "    model_results = {'Topics': [],\n",
    "                     'Alpha': [],\n",
    "                     'Beta': [],\n",
    "                     'Coherence': []}\n",
    "    \n",
    "    for num_topics in range(start, limit, step):\n",
    "        \n",
    "        # iterate through alpha values\n",
    "        for a in alpha:\n",
    "            \n",
    "            # iterare through beta values\n",
    "            for b in beta:\n",
    "                tfidf_lda_model = LdaMulticore(tfidf_corpus, # stream of document vectors \n",
    "                                       id2word=id2word, # mapping from word IDs to words\n",
    "                                       num_topics=num_topics,  # number of requested latent topics to be extracted from the training corpu\n",
    "                                       chunksize=100, # number of docs to be used in each training chunk\n",
    "                                       alpha=a, \n",
    "                                       eta=b, \n",
    "                                       random_state=42, # to ensure same result\n",
    "                                       eval_every=None, # don't evaluate model perplexity - takes too long\n",
    "                                       passes=2, # number of passes through the corpus during training\n",
    "                                       workers=4)\n",
    "        \n",
    "                tfidf_model_list.append(tfidf_lda_model)\n",
    "                coherencemodel = CoherenceModel(model=tfidf_lda_model, texts=text_data, dictionary=id2word, coherence='c_v')\n",
    "                coherence_value = coherencemodel.get_coherence()\n",
    "                coherence_values.append(coherence_value)\n",
    "                \n",
    "                # Save the model results\n",
    "                model_results['Topics'].append(num_topics)\n",
    "                model_results['Alpha'].append(a)\n",
    "                model_results['Beta'].append(b)\n",
    "                model_results['Coherence'].append(coherence_value)\n",
    "                \n",
    "    lda_tuning_result_df = pd.DataFrame(model_results)\n",
    "                \n",
    "    return tfidf_model_list, coherence_values, lda_tuning_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_doc_word_counts(df, doc_lens, ax_count, row_count, start_index=0):\n",
    "    '''\n",
    "    Plots distribution of document word counts.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Yelp's review or tip dataframe\n",
    "    doc_lens: list\n",
    "        list of word count per document\n",
    "    ax_count: int\n",
    "        # of axes per row\n",
    "    row_count: int\n",
    "        # of rows\n",
    "    start_index: int\n",
    "        starting index during loop\n",
    "    '''\n",
    "    colors = [color for name, color in mcolors.XKCD_COLORS.items()]\n",
    "    random.shuffle(colors)\n",
    "\n",
    "    #dpi = dots per inches - sharex/sharey - controls sharing of properties among x and y axes among all subplots \n",
    "    fig, axes = plt.subplots(ax_count, row_count, figsize=(15,10), dpi=100, sharex=True, sharey=True)\n",
    "\n",
    "    for i, ax in enumerate(axes.flatten(), start_index): # flatten - flattens axes group into individual ax\n",
    "\n",
    "        # Get individual topic dataframe\n",
    "        sub_df = df.loc[df['dominant_topic'] == i, :]\n",
    "\n",
    "        # Get word counts per document\n",
    "        doc_lens = [len(text.split()) for text in sub_df['clean_text']]\n",
    "\n",
    "        # Create histogram with 300 bins and specified random color\n",
    "        ax.hist(doc_lens, bins=300, color=colors[i])\n",
    "\n",
    "        # Set ticks on y-axis with set label color with specified random color\n",
    "        ax.tick_params(axis='y', labelcolor=colors[i], color=colors[i])\n",
    "\n",
    "        # Set kdeplot\n",
    "        sns.kdeplot(doc_lens, color=\"black\", shade=False, ax=ax.twinx())\n",
    "\n",
    "        ax.set(xlim=(0, 300), xlabel='Document Word Count')\n",
    "        ax.set_ylabel('Number of Documents', color=colors[i])\n",
    "        ax.set_title(f'Topic: {i}', fontdict=dict(size=15, color=colors[i]))\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.90)\n",
    "    plt.xticks(np.linspace(0,300,10))\n",
    "    fig.suptitle('Distribution of Document Word Counts by Dominant Topic', fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcounts(df, ax_count, row_count, start_index=0):\n",
    "    '''\n",
    "    Plot Word Count and Weights of Topic Keywords.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Dataframe consisting of topic keyword, topic id, weight (importance), and word counts\n",
    "    ax_count: int\n",
    "        # of axes per row\n",
    "    row_count: int\n",
    "        # of rows\n",
    "    start_index: int\n",
    "        starting index during loop\n",
    "    '''\n",
    "    # Get random colors\n",
    "    colors = [color for name, color in mcolors.XKCD_COLORS.items()]\n",
    "    random.shuffle(colors)\n",
    "    \n",
    "    fig, axes = plt.subplots(ax_count, row_count, figsize=(12,7), sharey=True, dpi=100)\n",
    "\n",
    "    for i, ax in enumerate(axes.flatten(), start_index):\n",
    "        \n",
    "        # Create bar graph - word count\n",
    "        ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=colors[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "        ax_twin = ax.twinx() # share bar graph\n",
    "        \n",
    "        # Weightage (importance)\n",
    "        ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=colors[i], width=0.2, label='Weights')\n",
    "        ax.set_ylabel('Word Count', color=colors[i])\n",
    "        \n",
    "        # Set y-axis view limits\n",
    "        ax.set_ylim(0, 5000000);\n",
    "        ax_twin.set_ylim(0, 0.030); \n",
    "\n",
    "        # Set title\n",
    "        ax.set_title(f'Topic: {i}', color=colors[i], fontsize=15)\n",
    "        ax.tick_params(axis='y', left=False)\n",
    "        \n",
    "        # Tilt x-labels 30 deg\n",
    "        ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment='right')\n",
    "        \n",
    "        # Set legend \n",
    "        ax.legend(loc='upper left'); \n",
    "        ax_twin.legend(loc='upper right')\n",
    "\n",
    "    fig.tight_layout(w_pad=2)    \n",
    "    fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=20, y=1.05)    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud(df, lda_model, ax_count, row_count, start_index=0):\n",
    "    '''\n",
    "    Shows top ten words by weight per topic.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Yelp's review or tip dataframe\n",
    "    lda_model: gensim.models.ldamulticore.LdaMulticore\n",
    "        LDA model that holds all topics\n",
    "    ax_count: int\n",
    "        # of axes per row\n",
    "    row_count: int\n",
    "        # of rows\n",
    "    start_index: int\n",
    "        starting index during loop\n",
    "    '''\n",
    "    colors = [color for name, color in mcolors.XKCD_COLORS.items()]\n",
    "    random.shuffle(colors)\n",
    "    \n",
    "    # instantiate word cloud\n",
    "    word_cloud = WordCloud(stopwords=stop_words,# using spacy's stopwords\n",
    "                           width=2500, # width of canvas\n",
    "                           height=1800, # height of canvas\n",
    "                           max_words=10, # Shows top 10 words by weight\n",
    "                           color_func=lambda *args, **kwargs: colors[i], # Sets color\n",
    "                           prefer_horizontal=1.0) # Horizontal fitting\n",
    "    \n",
    "    # formatted=false - returns 2 tuples of (word, probability)\n",
    "    # num_topics=-1 - shows all topics\n",
    "    topics = lda_model.show_topics(formatted=False, num_topics=-1) \n",
    "    \n",
    "    fig, axes = plt.subplots(ax_count, row_count, figsize=(15,10), sharex=True, sharey=True)\n",
    "\n",
    "    for i, ax in enumerate(axes.flatten(), start_index):\n",
    "        fig.add_subplot(ax)\n",
    "        topic_words = dict(topics[i][1])\n",
    "        word_cloud.generate_from_frequencies(topic_words, max_font_size=300)  # Creates word cloud based on frequencies\n",
    "        plt.gca().imshow(word_cloud) # imshow - display data as image\n",
    "        plt.gca().set_title(f'Topic {i}', fontdict=dict(size=15)) # setting title\n",
    "        plt.gca().axis('off') # removing axis\n",
    "\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)# remove spacing between axes\n",
    "    plt.axis('off')\n",
    "    plt.margins(x=0, y=0) # remove margins\n",
    "    plt.tight_layout() # automatically adjusts padding between and around subplots\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud(df, cuisine, col):\n",
    "    '''\n",
    "    Shows top ten words by weight per topic.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Dataframe with text and menu values\n",
    "    cuisine: str\n",
    "        cuisine type (ex: japanese, american, etc.)\n",
    "    col: str\n",
    "        column name\n",
    "    Returns:\n",
    "    -------\n",
    "    WordCloud: wordcloud.wordcloud.WordCloud\n",
    "        WordCloud object with 50 common words\n",
    "    '''\n",
    "    colors = [color for name, color in mcolors.XKCD_COLORS.items()]\n",
    "    random.shuffle(colors)\n",
    "    \n",
    "    vec = TfidfVectorizer(stop_words='english') # Instantiation\n",
    "    vecs = vec.fit_transform(df[df[cuisine] == 1][col]) # Learns vocularly and returns vectors based on tf-idf\n",
    "    \n",
    "    feature_names = vec.get_feature_names() # Gets text value\n",
    "    dense = vecs.todense() # Converts sparse matrix to dense matrix\n",
    "    \n",
    "    df = pd.DataFrame(dense.tolist(), columns=feature_names) # Create dataframe\n",
    "    transposed_series = df.T.sum(axis=1) # Switch row index as column index - vice versa and get column sums\n",
    "    \n",
    "    return WordCloud(max_words=25, \n",
    "                     color_func=lambda *args, **kwargs: colors[0], # Sets color\n",
    "                    ).generate_from_frequencies(transposed_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dominant_topics_and_perc(lda_model, tfidf_corpus, start_index, end_index):\n",
    "    '''\n",
    "    Gets dominant topics and topics percentage list.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lda_model: gensim.models.ldamulticore.LdaMulticore\n",
    "        LDA model that holds all topics\n",
    "    corpus: gensim.interfaces.TransformedCorpus\n",
    "        tfidf corpus\n",
    "    start_index: int\n",
    "        Starting index\n",
    "    end_index: int\n",
    "        Ending index\n",
    "    Returns\n",
    "    -------\n",
    "    dominant_topics: list of dominant topics\n",
    "        Topic id and dominant topic words\n",
    "    topic_percentages: list of topic percentages\n",
    "        Topic id and topic contribution percentage \n",
    "    '''\n",
    "    corpus_sel = tfidf_corpus[start_index:end_index]\n",
    "    dominant_topics = []\n",
    "    topic_percentages = []\n",
    "    \n",
    "    for i, corp in enumerate(corpus_sel): # [(0, 0.09631027287834815), (1, 0.12703629397704447), ...]\n",
    "        topic_percs, wordid_topics = lda_model[corp]\n",
    "        dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse=True)[0][0]\n",
    "        dominant_topics.append((i, dominant_topic))\n",
    "        topic_percentages.append(topic_percs)\n",
    "        \n",
    "    return(dominant_topics, topic_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_top_three_words_df(dominant_topics):\n",
    "    '''\n",
    "    Creates top three words per topic dataframe\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lda_model: list of dominant topics\n",
    "        Topic id and dominant topic words\n",
    "    Returns\n",
    "    -------\n",
    "    dominant_topics: list of dominant topics\n",
    "        Topic id and dominant topic words\n",
    "    '''\n",
    "    # Distribution of Dominant Topics in Each Document\n",
    "    df = pd.DataFrame(dominant_topics, columns=['Document_Id', 'Dominant_Topic'])\n",
    "    dominant_topic_in_each_doc = df.groupby('Dominant_Topic').size()\n",
    "    df_dominant_topic_in_each_doc = dominant_topic_in_each_doc.to_frame(name='count').reset_index()\n",
    "\n",
    "    # Total Topic Distribution by actual weight\n",
    "    topic_weightage_by_doc = pd.DataFrame([dict(t) for t in topic_percentages])\n",
    "    df_topic_weightage_by_doc = topic_weightage_by_doc.sum().to_frame(name='count').reset_index()\n",
    "\n",
    "    # Top 3 Keywords for each Topic\n",
    "    topic_top3words = [(i, topic) for i, topics in lda_model.show_topics(formatted=False) \n",
    "                                     for j, (topic, wt) in enumerate(topics) if j < 3]\n",
    "\n",
    "    df_top3words_stacked = pd.DataFrame(topic_top3words, columns=['topic_id', 'words'])\n",
    "    df_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\n",
    "    df_top3words.reset_index(level=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(x, index, n_gram):\n",
    "    '''\n",
    "    Returns top n words in unigram, bigram, or trigram.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: pandas.Series\n",
    "        Text values\n",
    "    index: int\n",
    "        Index which determines how many top n keywords we want\n",
    "    n_gram: int\n",
    "        Determines we want one or two-words or three-words \n",
    "    Returns\n",
    "    -------\n",
    "    words_freq: list of tuples\n",
    "         List of tuples with text as key and value as count\n",
    "    '''\n",
    "    vec = CountVectorizer(ngram_range=(n_gram, n_gram), stop_words='english').fit(x)\n",
    "    bow = vec.transform(x)\n",
    "    sum_words = bow.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0,idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(a, b):\n",
    "    '''\n",
    "    Defines cosine similarity distance\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a: list of dummy business attributes values (binary)\n",
    "    b: list of business attributes values from business dataframe(binary)\n",
    "    Returns\n",
    "    -------\n",
    "    sim: float\n",
    "        cosine similarity distance\n",
    "    '''\n",
    "    nom = np.sum(np.multiply(a, b))\n",
    "    denom = np.sqrt(np.sum(np.square(a))) * np.sqrt(np.sum(np.square(b)))\n",
    "    sim = nom / denom\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_w2v(doc_tokens, w2v_model):\n",
    "    '''\n",
    "    Generates and averages vectors for the whole document or query.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_tokens: list of str\n",
    "        list of text values\n",
    "    w2v_model: gensim.models.word2vec.Word2Vec\n",
    "        Word2Vec model\n",
    "    Returns\n",
    "    -------\n",
    "    vectors: list of floating points\n",
    "        300 vectors\n",
    "    '''\n",
    "    embeddings = []\n",
    "    \n",
    "    if len(doc_tokens) < 1:\n",
    "        # Any document or query of length zero will hae a vector containing zeros\n",
    "        return np.zeros(1000)\n",
    "    else:\n",
    "        # Iterates through all text values per document\n",
    "        for token in doc_tokens:\n",
    "            if token in w2v_model.wv.vocab:\n",
    "                # Get vectors for specific text value\n",
    "                embeddings.append(w2v_model.wv.word_vec(token))\n",
    "            else:\n",
    "                # When text value does not contain in w2v vocab - create 300 random vectors 0-1 \n",
    "                embeddings.append(np.random.rand(1000))\n",
    "        \n",
    "        # Return average vectors of indvidual words to get the vector of the documents\n",
    "        return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_ranking_ir(query, w2v_model, df, model_type):\n",
    "    '''\n",
    "    Generates query result dataframe based on ranking information retrieval.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query: str\n",
    "        Text value\n",
    "    w2v_model: gensim.models.word2vec.Word2Vec\n",
    "        Word2Vec model\n",
    "    df: pandas.DataFrame\n",
    "        Cleaned text value dataframe consisting of restaurant information\n",
    "    model_type: str\n",
    "        text or menu type (defining which w2v model is used)\n",
    "    Returns\n",
    "    -------\n",
    "    result_df: pandas.DataFrame\n",
    "        Dataframe with restaurant name and cosine similarity score\n",
    "    '''\n",
    "    \n",
    "    # Preprocess query\n",
    "    query = expand_contractions(query) # to ensure vocab uniformity with w2v vocab\n",
    "    query = tokenize(query) # lowercase, lemmatizes, and removes stop word\n",
    "    query = re.sub(' +', ' ', query) # Remove extra whitespaee\n",
    "    \n",
    "    # Generate vector\n",
    "    vector = get_embedding_w2v(query.split(), w2v_model) # split query sentence to separate words\n",
    "    \n",
    "    # Ranking documents\n",
    "    if model_type == 'text':\n",
    "        documents = df[['name', 'clean_text']].copy()\n",
    "    elif model_type == 'menu':\n",
    "        documents = df[['name', 'menu']].copy()\n",
    "    else:\n",
    "        documents = df[['name', 'combined_text']].copy()\n",
    "    \n",
    "    # Applies cosine similarity after reshaping np array into one single nested list\n",
    "    documents['similarity'] = df['vector'].apply(lambda x: cosine_similarity(np.array(vector).reshape(1,-1), \n",
    "                                                                             np.array(x).reshape(1, -1)).item())\n",
    "    \n",
    "    # Sort by similarity score\n",
    "    documents.sort_values(by='similarity', ascending=False, inplace=True)\n",
    "    \n",
    "    return documents.reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
